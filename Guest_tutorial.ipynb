{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, AutoModel\n",
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model_emb = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computing the attribute subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find the direction or subsapce within the embedding space that best encodes our attribute of interest. For gender, we will find the space that best encodes ***the difference*** between binary genders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select vocabulary that \"defines\" your attribute of interest\n",
    "\n",
    "# if using a template to create semantically bleached sentences as input, take care to choose words that create \n",
    "# coherent sentences\n",
    "# for binary gender, we can define semantic pairs (words that differ only in gender)\n",
    "\n",
    "def_pairs = [[\"mother\", \"father\"], \n",
    "             [\"woman\", \"man\"],\n",
    "             [\"girl\", \"boy\"], \n",
    "             [\"gal\", \"guy\"], \n",
    "             [\"lady\", \"gentleman\"], \n",
    "             [\"aunt\", \"uncle\"],\n",
    "             [\"grandmother\", \"grandfather\"],\n",
    "             [\"grandma\", \"grandpa\"],\n",
    "             [\"daughter\", \"son\"],       \n",
    "             [\"actress\", \"actor\"],\n",
    "             [\"waitress\", \"waiter\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word pair, obtain the difference vector between sentence representations\n",
    "\n",
    "diffs = []\n",
    "for pair in def_pairs:\n",
    "    # create input sentences\n",
    "    sentence_pair = [\"She is a \" + pair[0] + \".\", \"He is a \" + pair[1] + \".\"]\n",
    "    #sentence_pair = pair\n",
    "\n",
    "    # convert text sentence to BERT input\n",
    "    inputs = [tokenizer(s, return_tensors=\"pt\") for s in sentence_pair]\n",
    "\n",
    "    # pass input through BERT to obtain contextualized sentence representations\n",
    "    reps = []\n",
    "    for i in [0, 1]: # there are only 2 sentences for each pair\n",
    "        outputs = model_emb(**inputs[i])\n",
    "        cls_rep = outputs['last_hidden_state'][0][0]\n",
    "        sentence_repr = cls_rep.cpu().detach().numpy()\n",
    "        norm = np.linalg.norm(sentence_repr)\n",
    "        reps.append(sentence_repr/norm)\n",
    "    # store the difference vector for this pair of representations    \n",
    "    diff = reps[0] - reps[1]\n",
    "    diff_norm = np.linalg.norm(diff)\n",
    "    diffs.append(diff/diff_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 768)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we should have 11 difference vectors (one for each defined word pair)\n",
    "# and the length of each is the size of the model output (768 for base BERT)\n",
    "diffs = np.array(diffs)\n",
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25373462, 0.1690021 ], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit PCA for difference vectors and observe variance explained\n",
    "pca = PCA(n_components=2, random_state = 1)\n",
    "pca.fit(diffs)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the one-dimensional gender direction to be the first principal component\n",
    "g = pca.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying Hard Debias to gender neutral representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any sentence representation that is obtained through BERT can be hard debiased by projecting to the nullspace of the gender direction (i.e. make representations orthogonal to g in order to make them equally similar to both binary genders). Here we show an example application to some gender-neutral words of interest (occupations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some gender neutral words (taken from A1)\n",
    "\n",
    "gn_vocab = [\"nurse\", \n",
    "            \"assistant\",\n",
    "            \"housekeeper\",\n",
    "            \"hairdresser\",\n",
    "            \"nanny\",\n",
    "            \"director\",\n",
    "            \"programmer\",\n",
    "            \"software engineer\",\n",
    "            \"CEO\", \n",
    "            \"president\",\n",
    "            \"lawyer\",\n",
    "            \"doctor\",\n",
    "            \"teacher\",\n",
    "            \"pretty\",\n",
    "            \"beautiful\",\n",
    "            \"sweet\",\n",
    "            \"quiet\",\n",
    "            \"tough\",\n",
    "            \"mean\",\n",
    "            \"family\",\n",
    "            \"loyal\",\n",
    "            \"hero\",\n",
    "            \"hysterical\",\n",
    "            \"office\",\n",
    "            \"business\",\n",
    "            \"kitchen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take a word of interest, and obtain output sentence rep from BERT using single word input\n",
    "def sent_rep(sentence):\n",
    "    s_input = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    output = model_emb(**s_input)\n",
    "    cls_rep = output['last_hidden_state'][0][0]\n",
    "    sentence_repr = cls_rep.cpu().detach().numpy()\n",
    "    return sentence_repr/np.linalg.norm(sentence_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictiony between words and BERT representations\n",
    "word_reps = {}\n",
    "for w in gn_vocab:\n",
    "    word_reps[w] = sent_rep(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now can access representations for each word by lookup\n",
    "word_reps[\"CEO\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nurse 0.0038719177\n",
      "assistant 0.00054454803\n",
      "housekeeper 0.0026962757\n",
      "hairdresser 0.006020844\n",
      "nanny 0.0076822042\n",
      "teacher 0.0052980185\n",
      "pretty 0.0059719086\n",
      "beautiful 0.008090854\n",
      "sweet 0.008212507\n",
      "quiet 0.003807187\n",
      "family 0.0024880767\n",
      "hysterical 0.0012001395\n",
      "kitchen 0.0036526918\n"
     ]
    }
   ],
   "source": [
    "# check similarity between a job and gender subspace before debiasing \n",
    "# following experimental procedure from GG paper (use she-he axis)\n",
    "f = sent_rep(\"she\")\n",
    "m = sent_rep(\"he\")\n",
    "y = []\n",
    "for word, emb in word_reps.items():\n",
    "    # if more similar to she than he, assign F label\n",
    "    if np.inner(f, emb) > np.inner(m,emb):\n",
    "        print(word, np.inner(f, emb) - np.inner(m,emb))\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the \"debiased\" representations for each word\n",
    "deb_word_reps = {}\n",
    "X = []\n",
    "for word, emb in word_reps.items():\n",
    "    deb_emb = emb - np.inner(g, emb)*g\n",
    "    deb_word_reps[word] = deb_emb\n",
    "    \n",
    "    X.append(deb_emb)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6499947e-08"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check that the inner product (similarity) between gender neutral word and gender subspace is now ~0\n",
    "np.inner(deb_word_reps[\"teacher\"], g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 768)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias(emb, g):\n",
    "    deb_emb = emb - np.inner(g, emb)*g\n",
    "    return deb_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recoverability experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use gender names data, download \"name_gender_dataset.csv\" from https://archive.ics.uci.edu/ml/datasets/Gender+by+Name and place it in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\"name_gender_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Count</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td>M</td>\n",
       "      <td>5304407</td>\n",
       "      <td>0.014517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John</td>\n",
       "      <td>M</td>\n",
       "      <td>5260831</td>\n",
       "      <td>0.014398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td>M</td>\n",
       "      <td>4970386</td>\n",
       "      <td>0.013603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael</td>\n",
       "      <td>M</td>\n",
       "      <td>4579950</td>\n",
       "      <td>0.012534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William</td>\n",
       "      <td>M</td>\n",
       "      <td>4226608</td>\n",
       "      <td>0.011567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name Gender    Count  Probability\n",
       "0    James      M  5304407     0.014517\n",
       "1     John      M  5260831     0.014398\n",
       "2   Robert      M  4970386     0.013603\n",
       "3  Michael      M  4579950     0.012534\n",
       "4  William      M  4226608     0.011567"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(2000):\n",
    "    sent = names.loc[i, \"Name\"]\n",
    "    emb = sent_rep(sent)\n",
    "    deb_emb = hard_debias(emb, g)\n",
    "    X_train.append(deb_emb)\n",
    "    if names.loc[i, \"Gender\"] == \"F\":\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier.fit(X_train, y_train)\n",
    "#predictions = my_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = my_classifier.score(X,y) # predict gender labels given debiased representations of the gender-neutral words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846153846153846"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iterative Linear Nullspace Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of estimating the gender direction from our defined vocabulary and applying PCA, we learn the most informative decision boundary and project to the nullspace iteratively, until information cannot be recovered by a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our names data again construct a gender emb dataset, this time using their original (biased) representations\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(2000):\n",
    "    sent = names.loc[i, \"Name\"]\n",
    "    emb = sent_rep(sent)\n",
    "    X_train.append(emb)\n",
    "    if names.loc[i, \"Gender\"] == \"F\":\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a linear svm to classify the embs as either M or F \n",
    "# we can use the entire set for training as the only objective is to find a decision boundary\n",
    "# hold out a test set if you want to observe diminishing accuracy over iterations\n",
    "\n",
    "# we want a linear decision boundary that we can project \n",
    "my_clf = svm.LinearSVC()\n",
    "my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(my_clf.coef_) #obtain weights from classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_basis = orth(W.T) # orthogonal basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_W = w_basis.dot(w_basis.T) #ensure P is a projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_W.shape #projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debias w_deb = P*w\n",
    "def INLP(emb, P):\n",
    "    return P*emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for as many iterations as needed until classification accuracy looks like random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
